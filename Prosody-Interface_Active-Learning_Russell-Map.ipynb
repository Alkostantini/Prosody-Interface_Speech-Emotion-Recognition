{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"text-align: center; font-size: 36px; color: #3498db; font-weight: bold;\">Prosody Tools </h1>\n",
    "## <h2 style=\"text-align: center; font-size: 28px; color: #2ecc71; font-weight: bold;\">Prosody Interface</h2>\n",
    "### <h3 style=\"text-align: center; font-size: 24px; color: #e74c3c; font-family: 'Arial', sans-serif; font-weight: bold;\">Prosody Active Learning</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Project: Prosody\n",
    "\n",
    "Institution: Reutlingen University\n",
    "\n",
    "Author: Mohamad Eyad Alkostanini\n",
    "\n",
    "Supervisor: Bakir Hadzic, Parvez Mohammed, (ViSiR)\n",
    "\n",
    "Professor: Prof. Matthias RÃ¤tsch\n",
    "\n",
    "Description:\n",
    "This script performs diarization as part of the Prosody in Mental Health project.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Library\n",
    "##############\n",
    "\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import pyaudio\n",
    "import wave\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from datetime import datetime, timezone\n",
    "from tkinter import messagebox, Radiobutton\n",
    "import re\n",
    "import shutil\n",
    "import sounddevice as sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# loding the model, weights, scaler and encoder\n",
    "###############################################\n",
    "\n",
    "global PREDICTED_CLASS     \n",
    "OUTPUT_FILE=r\"./Output/input_voice.wav\"\n",
    "Output_folder= r\"./Output\"\n",
    "if not os.path.exists(Output_folder):    \n",
    "    os.makedirs(Output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\Alkostantini-GIGA\\.cache\\modelscope\\hub\\iic/emotion2vec_plus_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 22:59:16,352 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect model requirements, begin to install it: C:\\Users\\Alkostantini-GIGA\\.cache\\modelscope\\hub\\iic\\emotion2vec_plus_large\\requirements.txt\n",
      "install model requirements successfully\n",
      "ckpt: C:\\Users\\Alkostantini-GIGA\\.cache\\modelscope\\hub\\iic\\emotion2vec_plus_large\\model.pt\n",
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Functions section #\n",
    "#####################################\n",
    "\n",
    "\n",
    "\n",
    "from emotion2vecplus import Emotion2Vec\n",
    "emotion2vec = Emotion2Vec()\n",
    "######################\n",
    "# Prediction function\n",
    "######################\n",
    "def prediction(path):\n",
    "    # result = get_features(path)\n",
    "    # prediction = prosody_model.predict(result)\n",
    "    predicted_class, predicted_probs = emotion2vec.predict(path)\n",
    "    #y_prediction = prosody_encoder.inverse_transform(prediction.reshape(1, -1))\n",
    "    #predicted_class = y_prediction[0][0]\n",
    "    # class probabilities\n",
    "    #predicted_probs = prediction[0]\n",
    "    # class names from encoder\n",
    "    #class_names = prosody_encoder.categories_[0]\n",
    "    ''''\n",
    "    # Print predicted class and probabilities for all classes\n",
    "    print(\"Predictions for all classes:\")\n",
    "    for label, prob in zip(class_names, predicted_probs):\n",
    "        print(f\"{label}: {prob*100:.2f}%\")\n",
    "    '''\n",
    "    return predicted_class, predicted_probs\n",
    "\n",
    "\n",
    "###################\n",
    "# Database Function\n",
    "###################\n",
    "def df_database_function(database_folder):\n",
    "    \n",
    "    datagrams = []\n",
    "    for filename in os.listdir(database_folder):\n",
    "        if filename.endswith('.wav'):\n",
    "            emotion = filename.split('_')[0]\n",
    "            file_path = os.path.join(database_folder, filename)\n",
    "            datagram = {'path': file_path, 'Emotions': emotion}\n",
    "            datagrams.append(datagram)\n",
    "            \n",
    "    df0 = pd.DataFrame(datagrams)\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "# Model Fine-Tune \n",
    "############################\n",
    "\n",
    "def get_date_string():\n",
    "    current_datetime = datetime.now(timezone.utc)\n",
    "\n",
    "    # Format the datetime as desired\n",
    "    formatted_datetime = current_datetime.strftime('%Y_%m_%d_%H-%M')\n",
    "    return formatted_datetime\n",
    "\n",
    "def get_latest_experiment(experiments_dir = r\"tmp\"):\n",
    "\n",
    "    # List all folders in the experiments directory\n",
    "    experiment_folders = [folder for folder in os.listdir(experiments_dir)]\n",
    "    if not experiment_folders: #check if empty\n",
    "        return \"exp_\" + get_date_string()\n",
    "    \n",
    "    # Parse folder names and extract datetime information\n",
    "    parsed_folders = []\n",
    "    for folder_name in experiment_folders:\n",
    "        try:\n",
    "            folder_datetime = datetime.strptime(folder_name, 'exp_%Y_%m_%d_%H-%M')\n",
    "            parsed_folders.append((folder_datetime, folder_name))\n",
    "        except ValueError:\n",
    "            print(ValueError)\n",
    "            # Skip folders with names not matching the expected format\n",
    "            pass\n",
    "\n",
    "    # Sort the parsed folders based on datetime\n",
    "    sorted_folders = sorted(parsed_folders, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Retrieve the latest folder name\n",
    "    latest_folder = sorted_folders[0][1] if sorted_folders else None\n",
    "\n",
    "    print(\"Latest experiment folder:\", latest_folder)\n",
    "\n",
    "    return latest_folder\n",
    "\n",
    "new_experiment = True\n",
    "\n",
    "\n",
    "\n",
    "# database_folder = r\"./new_recordings\"\n",
    "# df_new = df_database_function(database_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"text-align: left; font-size: 28px; color: #2ecc71; font-weight: bold;\">Prosody Real-time & Prosody Active</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Prosody Real-Time & Prosody Active\n",
    "####################################\n",
    "# Ubdate prosody active Model\n",
    "###############################\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "######################################################################################    \n",
    "\n",
    "class EmotionPlotter:\n",
    "    def __init__(self, root, update_callback):\n",
    "        self.root = root\n",
    "        self.root.lift()\n",
    "        self.root.title(\"Prosody\")\n",
    "        \n",
    "        frame_color= '#F5F5F5'\n",
    "        #self.root.configure(bg='#2c3e50')\n",
    "        self.root.configure(bg=frame_color)\n",
    "        self.update_callback = update_callback\n",
    "        self.class_names = ['Noise', 'angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "        self.prediction_history = []  # List to store history of predictions\n",
    "\n",
    "\n",
    "\n",
    "        # Create subplots for the histogram and the valence-arousal map\n",
    "        self.fig, (self.ax, self.ax_russell) = plt.subplots(1, 2, figsize=(18, 9))  \n",
    "        # Configure histogram axis\n",
    "        self.ax.set_title('Emotion Prediction (Histogram)')\n",
    "        self.ax.set_xlabel('Emotion')\n",
    "        self.ax.set_ylabel('Probability (%)')\n",
    "        self.ax.set_ylim(0, 100)\n",
    "        self.ax.set_xticks(np.arange(len(self.class_names)))\n",
    "        self.ax.set_xticklabels(self.class_names, rotation=0)\n",
    "\n",
    "        # Configure valence-arousal Russell map axis\n",
    "        self.ax_russell.set_title('Valence-Arousal Map (Russell Map)')\n",
    "        self.ax_russell.set_xlim(-1, 1)\n",
    "        self.ax_russell.set_ylim(-1, 1)\n",
    "        self.ax_russell.set_xlabel('Valence')\n",
    "        self.ax_russell.set_ylabel('Arousal')\n",
    "        self.ax_russell.axhline(0, color='gray', lw=1)\n",
    "        self.ax_russell.axvline(0, color='gray', lw=1)\n",
    "\n",
    "        # self.canvas = FigureCanvasTkAgg(self.fig, master=self.root)\n",
    "        # self.canvas.draw()\n",
    "        # self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=1)\n",
    "\n",
    "\n",
    "\n",
    "        # # Create a matplotlib figure and axis\n",
    "        # self.fig, self.ax = plt.subplots()\n",
    "\n",
    "        # # Embed the plot in the Tkinter window\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=self.root)\n",
    "        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Optional: display the chart initially\n",
    "        self.ax.bar([0, 1, 2], [0, 0, 0])  # Initialize with empty chart\n",
    "        self.canvas.draw()\n",
    "\n",
    "        #########################\n",
    "        # Recording configuration\n",
    "        #########################\n",
    "        self.RECORD_SECONDS = 3  # recording duration\n",
    "        self.FORMAT = pyaudio.paInt16\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 44100\n",
    "        self.CHUNK = 1024\n",
    "        self.CHUNK_SIZE = 1024\n",
    "        self.OUTPUT_FILE = OUTPUT_FILE\n",
    "        #######################################################################\n",
    "        #########################\n",
    "        # buttons configuration\n",
    "        ##########################\n",
    "        # Button styles\n",
    "        button_font = ('Helvetica', 14, 'bold')\n",
    "        button_style = {\n",
    "            'font': button_font,\n",
    "            'bg': '#2c3e60',  # Green background\n",
    "            'fg': 'white',    # White text\n",
    "            'activebackground': '#2c3e55',  # Darker green when pressed\n",
    "            'activeforeground': 'white',    # White text when pressed\n",
    "            'borderwidth': 0,\n",
    "            'padx': 10,\n",
    "            'pady': 5\n",
    "        }\n",
    "\n",
    "        # Create a frame for the buttons\n",
    "        button_frame = tk.Frame(self.root, bg=frame_color)\n",
    "        button_frame.pack(side=tk.TOP, pady=(10, 10))\n",
    "\n",
    "        # Start button\n",
    "        self.start_button = tk.Button(\n",
    "            button_frame, text=\"Start Recording\",\n",
    "            command=self.start_visualization,\n",
    "            **button_style\n",
    "        )\n",
    "        self.start_button.pack(side=tk.LEFT, padx=10, pady=5)\n",
    "\n",
    "        # Continue button\n",
    "        self.continue_button = tk.Button(\n",
    "            button_frame, text=\"Active Mode\",\n",
    "            command=self.active_mode,\n",
    "            **button_style\n",
    "        )\n",
    "        self.continue_button.pack(side=tk.LEFT, padx=10, pady=5)\n",
    "        self.continue_button['state'] = tk.DISABLED\n",
    "\n",
    "        # Exit button\n",
    "        self.exit_button = tk.Button(\n",
    "            button_frame, text=\"Exit\",\n",
    "            command=self.root.destroy,\n",
    "            **button_style\n",
    "        )\n",
    "        self.exit_button.pack(side=tk.RIGHT, padx=10, pady=5)\n",
    "        self.exit_button['state'] = tk.NORMAL\n",
    "\n",
    "        # Listen button\n",
    "        self.listen_button = tk.Button(\n",
    "            button_frame, text=\"Play the Voice\",\n",
    "            command=self.play_voice,\n",
    "            **button_style\n",
    "        )\n",
    "        self.listen_button.pack(side=tk.RIGHT, padx=10, pady=5)\n",
    "        self.listen_button['state'] = tk.DISABLED\n",
    "        self.visualizing = False                 # track of visualization state\n",
    "        self.root.after(0, self.center_window)  \n",
    "\n",
    "\n",
    "\n",
    "    \t# # Continuous\n",
    "        # self.continuous_button = tk.Button(\n",
    "        #     button_frame, text=\"Continuous Prediction\", \n",
    "        #     command=self.start_continuous_prediction,\n",
    "        #     **button_style\n",
    "        # )\n",
    "        # self.continuous_button.pack(side=tk.LEFT, padx=10, pady=5)\n",
    "        # self.continuous_button['state'] = tk.NORMAL  # Enable the button   \n",
    "\n",
    "        # Exit button\n",
    "        # self.exit_button = tk.Button(\n",
    "        #     button_frame, text=\"Stop Continuous Mode\",\n",
    "        #     command=self.stop_continuous_mode,\n",
    "        #     **button_style\n",
    "        # )\n",
    "        # self.exit_button.pack(side=tk.RIGHT, padx=10, pady=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def center_window(self):\n",
    "        # self.root.update_idletasks()  \n",
    "        # width = self.root.winfo_width()\n",
    "        # height = self.root.winfo_height()\n",
    "        # x = (self.root.winfo_screenwidth() // 2) - (width // 2)\n",
    "        # y = (self.root.winfo_screenheight() // 2) - (height // 2)\n",
    "        # self.root.geometry(f'{width}x{height}+{x}+{y}')\n",
    "\n",
    "        self.root.update_idletasks()  # Update \"idle\" tasks to get the current size.\n",
    "        # Set fullscreen\n",
    "        self.root.attributes('-fullscreen', True)\n",
    "        # Optionally, you can bind 'Esc' to exit fullscreen\n",
    "        self.root.bind(\"<Escape>\", lambda event: self.root.attributes('-fullscreen', False))\n",
    "        \n",
    "    def update_callback(self, predicted_class, predicted_probs):\n",
    "        \"\"\"Update the chart based on the predicted class and probabilities.\"\"\"\n",
    "        \n",
    "        # Clear the current plot\n",
    "        self.ax.clear()\n",
    "\n",
    "        # Plot updated probabilities\n",
    "        self.ax.bar(range(len(predicted_probs)), predicted_probs)\n",
    "\n",
    "        # Update the title or labels as needed\n",
    "        self.ax.set_title(f'Predicted Class: {predicted_class}')\n",
    "        self.ax.set_xlabel('Emotion Classes')\n",
    "        self.ax.set_ylabel('Probability')\n",
    "\n",
    "        # Redraw the canvas to reflect the changes\n",
    "        self.canvas.draw()\n",
    "    ############################################################\n",
    "    ############################        \n",
    "    # buttons update\n",
    "    ##########################\n",
    "    def start_continuous_prediction(self):\n",
    "        \"\"\"Start continuous prediction loop.\"\"\"\n",
    "        self.visualizing = True\n",
    "        self.start_button['state'] = tk.DISABLED\n",
    "        self.continue_button['state'] = tk.DISABLED\n",
    "        self.listen_button['state'] = tk.DISABLED\n",
    "        self.continuous_button['state'] = tk.DISABLED\n",
    "        self.exit_button['state'] = tk.DISABLED\n",
    "        self.root.update()\n",
    "        self.canvas.draw()\n",
    "\n",
    "        # Start continuous update loop\n",
    "        self.continuous_prediction_loop()\n",
    "\n",
    "    def continuous_prediction_loop(self):\n",
    "        \"\"\"Loop that handles continuous prediction and chart update.\"\"\"\n",
    "        if self.visualizing: \n",
    "            # Perform a new prediction\n",
    "            global PREDICTED_CLASS\n",
    "            self.root.update()\n",
    "\n",
    "            PREDICTED_CLASS, predicted_probs = prediction(self.OUTPUT_FILE)\n",
    "            self.root.update()\n",
    "\n",
    "            # Update the chart with the latest prediction\n",
    "            self.update_callback(self, PREDICTED_CLASS, predicted_probs)\n",
    "            # Trigger redrawing of the chart\n",
    "            self.canvas.draw()\n",
    "            self.root.update()\n",
    "\n",
    "            # Repeat this after 3 seconds \n",
    "            self.root.after(3000, self.continuous_prediction_loop)\n",
    "            self.root.update()\n",
    "\n",
    "        \n",
    "\n",
    "    def stop_continuous_mode(self):\n",
    "        \"\"\"Stop continuous prediction and enable other buttons.\"\"\"\n",
    "        self.visualizing = False  # Stop the continuous prediction loop\n",
    "        self.start_button['state'] = tk.NORMAL\n",
    "        self.continue_button['state'] = tk.NORMAL\n",
    "        self.listen_button['state'] = tk.NORMAL\n",
    "        self.continuous_button['state'] = tk.NORMAL\n",
    "        self.exit_button['state'] = tk.NORMAL\n",
    "        self.exit_button.config(text=\"Exit\")\n",
    "\n",
    "\n",
    "\n",
    "    def start_visualization(self):\n",
    "        if not self.visualizing:\n",
    "            self.visualizing = True\n",
    "            self.start_button['state'] = tk.DISABLED\n",
    "            self.continue_button['state'] = tk.DISABLED\n",
    "            self.listen_button['state'] = tk.DISABLED\n",
    "            self.update_start_loop()\n",
    "    \n",
    "    def active_mode(self):\n",
    "        if not self.visualizing:\n",
    "            self.visualizing = True\n",
    "            self.start_button['state'] = tk.DISABLED\n",
    "            self.continue_button['state'] = tk.DISABLED\n",
    "            self.listen_button['state'] = tk.DISABLED\n",
    "            self.exit_button['state'] = tk.DISABLED\n",
    "            self.visualizing = False       \n",
    "        #OUTPUT_FILE=r\"./Output/input_voice.wav\"\n",
    "        database_folder= r'./Database'\n",
    "        self.Prosody_active(self.OUTPUT_FILE, database_folder, self.start_button, self.continue_button, self.listen_button, self.exit_button).active_GUI()\n",
    "          \n",
    "    def play_voice(self):\n",
    "\n",
    "        #playsound(OUTPUT_FILE)\n",
    "        # sound = AudioSegment.from_wav(OUTPUT_FILE)\n",
    "        # play(sound)\n",
    "        # self.root.update()\n",
    "        sound = AudioSegment.from_wav(self.OUTPUT_FILE)\n",
    "\n",
    "        audio_data = np.array(sound.get_array_of_samples(), dtype=np.int16)\n",
    "\n",
    "        # Play the audio using sounddevice\n",
    "        sd.play(audio_data, samplerate=sound.frame_rate)\n",
    "        sd.wait()  # Wait until the sound is finished playing\n",
    "    \n",
    "    def update_start_text(self, text):\n",
    "        self.start_button.config(text=text)\n",
    "        self.root.update()\n",
    "        \n",
    "    def update_continue_text(self, text):\n",
    "        self.continue_button.config(text=text)\n",
    "        self.root.update()   \n",
    "             \n",
    "    def update_start_loop(self):\n",
    "        ########################\n",
    "        # Start recording audio\n",
    "        ########################\n",
    "        self.update_start_text(\"Recording in Progress...\")\n",
    "        print(\"Listening...\")\n",
    "        audio = pyaudio.PyAudio()\n",
    "        stream = audio.open(format=self.FORMAT, channels=self.CHANNELS, rate=self.RATE, input=True, frames_per_buffer=self.CHUNK)\n",
    "        frames = []\n",
    "        # Recording \n",
    "        for i in range(0, int(self.RATE / self.CHUNK_SIZE * self.RECORD_SECONDS)):\n",
    "            data = stream.read(self.CHUNK_SIZE)\n",
    "            frames.append(data)\n",
    "        # Stop recording\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "        # Saving audio\n",
    "        with wave.open(self.OUTPUT_FILE, 'wb') as wf:\n",
    "            wf.setnchannels(self.CHANNELS)\n",
    "            wf.setsampwidth(audio.get_sample_size(self.FORMAT))\n",
    "            wf.setframerate(self.RATE)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "             \n",
    "        ######################\n",
    "        # Perform prediction\n",
    "        ######################\n",
    "        self.update_start_text(\"Restart Recording\")\n",
    "        #self.update_continue(\"active Model\")\n",
    "        self.start_button['state'] = tk.NORMAL\n",
    "        self.continue_button['state'] = tk.NORMAL\n",
    "        self.listen_button['state'] = tk.NORMAL\n",
    "        #######################\n",
    "        global PREDICTED_CLASS\n",
    "        #######################\n",
    "        PREDICTED_CLASS, predicted_probs = prediction(self.OUTPUT_FILE)\n",
    "        # callback\n",
    "        self.update_callback(self, PREDICTED_CLASS, predicted_probs)\n",
    "        # Reset visualization\n",
    "        self.visualizing = False\n",
    "        self.root.update() \n",
    "    ##################################################################\n",
    "    ########################\n",
    "    # Update Plot\n",
    "    ########################\n",
    "    def update_plot(self, predicted_probs):\n",
    "\n",
    "        # Update Histogram\n",
    "        self.ax.clear()\n",
    "        bars = self.ax.bar(self.class_names, predicted_probs * 100, color='blue')\n",
    "        self.ax.set_title('Emotion Prediction (Histogram)')\n",
    "        self.ax.set_xlabel('Emotion')\n",
    "        self.ax.set_ylabel('Probability (%)')\n",
    "        self.ax.set_ylim(0, 100)\n",
    "        self.ax.set_xticks(np.arange(len(self.class_names)))\n",
    "        self.ax.set_xticklabels(self.class_names, rotation=0)\n",
    "\n",
    "        for i, bar in enumerate(bars):\n",
    "            self.ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{predicted_probs[i]*100:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "        # Update Russell Valence-Arousal Map\n",
    "        # Russel's map table\n",
    "        data = {\n",
    "            \"mood\": [\n",
    "                \"sleepy & tired\", \".\", \"afraid\", \"angry\", \"calm\", \"relaxed\", \"content\",\n",
    "                \"depressed\", \"discontent\", \"determined\", \"happy\", \"anxious\", \"good\",\n",
    "                \"pensive\", \"impressed\", \"frustrated\", \"disappointed\", \"bored\", \"annoyed\",\n",
    "                \"enraged\", \"excited\", \"melancholy\", \"satisfied\", \"distressed\",\n",
    "                \"uncomfortable\", \"worried\", \"amused\", \"apathetic\", \"peaceful\",\n",
    "                \"contemplative\", \"embarrassed\", \"sad\", \"hopeful\", \"pleased\", \"None\"\n",
    "            ],\n",
    "            \"valence\": [\n",
    "                0.01, -0.01, -0.12, -0.40, 0.78, 0.71, 0.81, -0.81, -0.68, 0.73, 0.89, -0.72, 0.90,\n",
    "                0.09, 0.39, -0.60, -0.80, -0.35, -0.44, -0.18, 0.70, -0.05, 0.77, -0.71,\n",
    "                -0.68, -0.07, 0.55, -0.20, 0.55, 0.58, -0.31, -0.81, 0.61, 0.89, 0.00\n",
    "            ],\n",
    "            \"arousal\": [\n",
    "                -1.00, -1.00, 0.79, 0.79, -0.68, -0.65, -0.55, -0.48, -0.32, 0.26, 0.17, -0.80, -0.08,\n",
    "                -0.60, -0.06, 0.40, -0.03, -0.78, 0.76, 0.83, 0.71, -0.65, -0.63, 0.55,\n",
    "                -0.37, -0.32, 0.19, -0.12, -0.80, -0.60, -0.60, -0.40, -0.30, -0.10, 0.00\n",
    "            ],\n",
    "            \"#posts\": [\n",
    "                11549, 20308, 20, 3152, 10040, 3545, 11177, 6383, 2490, 3360, 16518, 7039, 5062,\n",
    "                2322, 1610, 4356, 3328, 12784, 8247, 1016, 11074, 2274, 2629, 2233,\n",
    "                1763, 3252, 24231, 2732, 2513, 10718, 1092, 6119, 5312, 3517, 3000\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        self.ax_russell.clear()\n",
    "        self.ax_russell.set_title('Emotion Prediction (Russell Map)')\n",
    "        self.ax_russell.set_xlim(-1, 1)\n",
    "        self.ax_russell.set_ylim(-1, 1)\n",
    "        self.ax_russell.axhline(0, color='gray', lw=1)\n",
    "        self.ax_russell.axvline(0, color='gray', lw=1)\n",
    "\n",
    "        emotion_coordinates = {\n",
    "            \"angry\": (-0.40, 0.79),\n",
    "            \"disgust\": (-0.68, -0.37),  \n",
    "            \"fear\": (-0.12, 0.79),     \n",
    "            \"happy\": (0.89, 0.17),\n",
    "            \"neutral\": (0.00, 0.00),\n",
    "            \"sad\": (-0.81, -0.40),\n",
    "            \"surprise\": (0.70, 0.71)  \n",
    "        }\n",
    "\n",
    "\n",
    "    #         # coordinates for the 7 emotions\n",
    "    # emotion_coordinates = {\n",
    "    #     \"Noise\": (0.00, 0.00),\n",
    "    #     \"angry\": (-0.40, 0.79),\n",
    "    #     \"disgust\": (-0.68, -0.37),  # uncomfortable\n",
    "    #     \"fear\": (-0.12, 0.79),     # afraid\n",
    "    #     \"happy\": (0.89, 0.17),\n",
    "    #     #\"neutral\": (0.78, -0.68),  # calm\n",
    "    #     \"neutral\": (0.00, 0.00),\n",
    "    #     \"sad\": (-0.81, -0.40),\n",
    "    #     \"surprise\": (0.70, 0.71)   # excited\n",
    "    # }\n",
    "\n",
    "\n",
    "        # Calculate weighted valence and arousal\n",
    "        weighted_valence = 0\n",
    "        weighted_arousal = 0\n",
    "        total_percentage = sum(predicted_probs[1:])  # Excluding 'Noise'\n",
    "\n",
    "        for i, emotion in enumerate(self.class_names[1:]):  # Skip 'Noise'\n",
    "            valence, arousal = emotion_coordinates.get(emotion, (0, 0))\n",
    "            weighted_valence += valence * predicted_probs[i + 1]  # Adjust index for skipped 'Noise'\n",
    "            weighted_arousal += arousal * predicted_probs[i + 1]\n",
    "        \n",
    "        weighted_valence /= total_percentage\n",
    "        weighted_arousal /= total_percentage\n",
    "\n",
    "        # Store the current prediction\n",
    "        self.prediction_history.append((weighted_valence, weighted_arousal))\n",
    "\n",
    "\n",
    "        # Clear current plots\n",
    "        self.ax_russell.clear()\n",
    "        self.ax_russell.set_title('Emotion Prediction (Russell Map)')\n",
    "        self.ax_russell.set_xlim(-1, 1)\n",
    "        self.ax_russell.set_ylim(-1, 1)\n",
    "        self.ax_russell.axhline(0, color='gray', lw=1)\n",
    "        self.ax_russell.axvline(0, color='gray', lw=1)\n",
    "\n",
    "        # Plot historical points\n",
    "        for valence, arousal in self.prediction_history:\n",
    "            self.ax_russell.scatter(valence, arousal, color='orange', s=50, alpha=0.5)\n",
    "\n",
    "\n",
    "        # Plot the weighted valence-arousal point\n",
    "        self.ax_russell.scatter(weighted_valence, weighted_arousal, color='red', s=100, label='Predicted Emotion')\n",
    "\n",
    "        self.ax_russell.scatter(df['valence'], df['arousal'], s=df['#posts'], alpha=0.5, label='Moods')  # Posts the area of the points\n",
    "        for i, row in df.iterrows():\n",
    "            plt.text(row['valence'], row['arousal'], row['mood'], fontsize=9, ha='right')\n",
    "\n",
    "        # # Add emotion coordinates for reference\n",
    "        # for emotion, (valence, arousal) in emotion_coordinates.items():\n",
    "        #     self.ax_russell.scatter(valence, arousal, label=emotion, color='orange')\n",
    "        #     self.ax_russell.text(valence + 0.02, arousal + 0.02, emotion, fontsize=10)\n",
    "\n",
    "        #self.ax_russell.legend(loc='upper right')\n",
    "        \n",
    "        # Redraw canvas\n",
    "        self.canvas.draw()\n",
    "   \n",
    "    #######################################################################################\n",
    "    #######################################################################################\n",
    "    # active-learning      \n",
    "    #######################################################################################\n",
    "    #######################################################################################\n",
    "    class Prosody_active:\n",
    "        def __init__ (self, output_file, output_folder, start_button, continue_button, listen_button,exit_button):\n",
    "            self.start_button = start_button\n",
    "            self.continue_button = continue_button\n",
    "            self.listen_button = listen_button\n",
    "            self.exit_button = exit_button\n",
    "            \n",
    "            self.OUTPUT_FILE = output_file\n",
    "            self.database_folder = output_folder\n",
    "            self.predicted_class = PREDICTED_CLASS\n",
    "            self.classes = ['Noise', 'angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "            self.selected_label = tk.StringVar()\n",
    "            self.root = tk.Toplevel()               # giving active Gui Root other level as main Gui Root\n",
    "            self.root.title(\"Prosody Active-Learning\")\n",
    "            self.root.geometry(\"700x200\")\n",
    "            self.selected_label.set(None)\n",
    "            self.root.protocol(\"WM_DELETE_WINDOW\", self.on_window_close)\n",
    "            self.root.after(0, self.center_window)\n",
    "            \n",
    "        def center_window(self):\n",
    "            self.root.update_idletasks()  \n",
    "            width = self.root.winfo_width()\n",
    "            height = self.root.winfo_height()\n",
    "            x = (self.root.winfo_screenwidth() // 2) - (width // 2)\n",
    "            y = (self.root.winfo_screenheight() // 2) - (height // 2)\n",
    "            self.root.geometry(f'{width}x{height}+{x}+{y}')\n",
    "\n",
    "        def on_window_close(self):\n",
    "                # Reset button states when window is closed\n",
    "                self.start_button['state'] = tk.NORMAL\n",
    "                self.continue_button['state'] = tk.NORMAL\n",
    "                self.listen_button['state'] = tk.NORMAL\n",
    "                self.exit_button['state'] = tk.NORMAL\n",
    "                # Destroy the window\n",
    "                self.root.destroy()\n",
    "        #######################\n",
    "        # Data Folder structure\n",
    "        #######################       \n",
    "        def save_data(self, selected_value):    \n",
    "            if not os.path.exists(self.database_folder):\n",
    "                os.makedirs(self.database_folder)\n",
    "                \n",
    "            existing_files = [f for f in os.listdir(self.database_folder) if f.endswith('.wav')]\n",
    "            ##########################\n",
    "            # Find the max file value\n",
    "            ##########################\n",
    "            max_number = 0\n",
    "            for file_name in existing_files:\n",
    "                match = re.match(rf\"{selected_value}_(\\d+)\\.wav\", file_name)\n",
    "                if match:\n",
    "                    number = int(match.group(1))\n",
    "                    max_number = max(max_number, number)\n",
    "            ###########################   \n",
    "            # save the file\n",
    "            ###############\n",
    "            new_file_name = f\"{selected_value}_{max_number + 1}.wav\"\n",
    "            output_file_path = os.path.join(self.database_folder, new_file_name)\n",
    "            shutil.copyfile(self.OUTPUT_FILE, output_file_path)\n",
    "            print('saved to Database')\n",
    "            ############################\n",
    "            messagebox.showinfo(\"Success\", \"Information saved to Prosody Database.\")\n",
    "        #####################################################################################\n",
    "        ############################\n",
    "        # Fine Tune\n",
    "        #############################\n",
    "        # def fine_tune(self, correct_label):\n",
    "        \n",
    "        #     print ('correct_label: ', correct_label)\n",
    "            \n",
    "        #     df_database = pd.DataFrame()\n",
    "        #     df_database = df_database_function(self.database_folder)\n",
    "        #     df_database.to_csv(r'./Models/df_Database.csv', index = False)\n",
    "        #     model_finetune(df_database)\n",
    "\n",
    "        #     messagebox.showinfo(\"Success\", \"Model retrained with the new information.\")\n",
    "        #     self.root.destroy()\n",
    "        #################################################################################            \n",
    "        ###########################\n",
    "        # Save without training\n",
    "        ###########################\n",
    "        def B_save_data(self):\n",
    "            selected_value = self.selected_label.get()\n",
    "            if selected_value in self.classes:\n",
    "                self.save_data(selected_value)\n",
    "                print('Data saved without training. Selected label: ', selected_value)\n",
    "\n",
    "                self.start_button['state'] = tk.NORMAL\n",
    "                self.continue_button['state'] = tk.NORMAL\n",
    "                self.listen_button['state'] = tk.NORMAL\n",
    "                self.exit_button['state'] = tk.NORMAL\n",
    "                self.visualizing = False\n",
    "                self.root.update()\n",
    "                \n",
    "                if tk._default_root is not None:  # check root window exists\n",
    "                    self.root.destroy()\n",
    "        #########################################################################            \n",
    "        ############################# \n",
    "        # Save and Retrain the model\n",
    "        #############################\n",
    "        def B_save_retrain(self):\n",
    "            self.start_button.config(text='Retraining')\n",
    "            self.continue_button.config(text='in')\n",
    "            self.listen_button.config(text='progress')\n",
    "            self.exit_button.config(text='...')\n",
    "            \n",
    "            selected_value = self.selected_label.get()\n",
    "            if selected_value in self.classes:\n",
    "                self.save_data(selected_value)\n",
    "                self.fine_tune(selected_value)\n",
    "                print('Selected label: ', selected_value)\n",
    "                \n",
    "                self.start_button['state'] = tk.NORMAL\n",
    "                self.continue_button['state'] = tk.NORMAL\n",
    "                self.listen_button['state'] = tk.NORMAL\n",
    "                self.exit_button['state'] = tk.NORMAL\n",
    "                \n",
    "                self.start_button.config(text='Restart Recording')\n",
    "                self.continue_button.config(text='Active Mode')\n",
    "                self.listen_button.config(text='Play the Voice')\n",
    "                self.exit_button.config(text='Exit')\n",
    "                \n",
    "                self.visualizing = False\n",
    "                self.root.update()\n",
    "                \n",
    "                if tk._default_root is not None:  # check root window exists\n",
    "                    self.root.destroy()\n",
    "        ########################################################################            \n",
    "        ###########################\n",
    "        # Bottums & Perform text \n",
    "        ###########################\n",
    "        def active_GUI(self):\n",
    "            #self.selected_label.set(None)\n",
    "            predicted_label_text = tk.StringVar()\n",
    "            predicted_label_text.set(\"Predicted Class: \" + self.predicted_class + \"\\n Correct the Class\")\n",
    "            predicted_label_label = tk.Label(self.root, textvariable=predicted_label_text, font=(\"Arial\", 16))\n",
    "            predicted_label_label.pack()\n",
    "            checkbox_frame = tk.Frame(self.root)\n",
    "            checkbox_frame.pack(padx=10, pady=10)\n",
    "            for label in self.classes:\n",
    "                radio_button = Radiobutton(checkbox_frame, text=label, variable=self.selected_label, value=label,\n",
    "                                        font=(\"Arial\", 14))\n",
    "                radio_button.pack(side=tk.LEFT, padx=10)\n",
    "            save_button = tk.Button(self.root, text=\"Save to Database\", command=self.B_save_data, font=(\"Arial\", 14))\n",
    "            save_button.pack()\n",
    "\n",
    "            save_retrain_button = tk.Button(self.root, text=\"Save to Database and retrain the model\",\n",
    "                                            command=self.B_save_retrain, font=(\"Arial\", 14))\n",
    "            save_retrain_button.pack()\n",
    "\n",
    "            self.root.mainloop()\n",
    "            \n",
    "            \n",
    "def visualize_emotion_prediction(plotter, predicted_class, predicted_probs):\n",
    "    plotter.update_plot(predicted_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.176: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  1.89it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.109: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  3.04it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.109: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  3.04it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.105: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  3.16it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.123: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  2.70it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼å¿/happy\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.128: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  2.60it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.108: 100%|\u001b[34mââââââââââ\u001b[0m| 1/1 [00:00<00:00,  3.06it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¾è¿/sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alkostantini-GIGA\\anaconda3\\envs\\interface\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Alkostantini-GIGA\\AppData\\Local\\Temp\\ipykernel_27000\\1437577428.py\", line 252, in active_mode\n",
      "    self.Prosody_active(self.OUTPUT_FILE, database_folder, self.start_button, self.continue_button, self.listen_button, self.exit_button).active_GUI()\n",
      "  File \"C:\\Users\\Alkostantini-GIGA\\AppData\\Local\\Temp\\ipykernel_27000\\1437577428.py\", line 600, in active_GUI\n",
      "    self.root.mainloop()\n",
      "  File \"c:\\Users\\Alkostantini-GIGA\\anaconda3\\envs\\interface\\lib\\tkinter\\__init__.py\", line 1429, in mainloop\n",
      "    self.tk.mainloop(n)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # callback function\n",
    "    root = tk.Tk() #root window\n",
    "    ####################################\n",
    "    # Ensure the window stays on top initially\n",
    "    root.attributes(\"-topmost\", True)\n",
    "    root.after(0, lambda: root.attributes(\"-topmost\", False))\n",
    "\n",
    "    \n",
    "    # root.lift()\n",
    "    #####################################\n",
    "    \n",
    "    \n",
    "    plotter = EmotionPlotter(root, visualize_emotion_prediction)\n",
    "    #plotter = EmotionPlotter(root, update_callback=None)\n",
    "\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'.\\Models\\df_Database.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
